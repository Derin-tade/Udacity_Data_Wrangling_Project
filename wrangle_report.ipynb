{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aad7a17",
   "metadata": {},
   "source": [
    "# Reporting: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297db61c",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd151b4",
   "metadata": {},
   "source": [
    "The expression \"enjoy the ride\" took on more meaning for me when I realized how wrangling could be divided into smaller procedures and sub-processes. Here is a summary of my data wrangling process, from gathering to assessing to cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6fcae",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf27aa0",
   "metadata": {},
   "source": [
    "This project required us to gather our data in 3 different ways.<br>\n",
    "- Loading the twitter_archive_enhanced csv_file into a dataframe in our notebook.\n",
    "- Downloading the image_prediction table programmatically using `request.get`.\n",
    "- Querying the Twitter API for each tweet's JSON data using Python's Tweepy library and storing each tweet's JSON data in a file called tweet_json.txt file. Then using this json.txt file to create a dataframe with tweet_id, retweet_count and favourite_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcee83",
   "metadata": {},
   "source": [
    "Below are some code snippets that helped me achieve the tasks above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60870f3b",
   "metadata": {},
   "source": [
    "```twitter_arc = pd.read_csv(\"twitter-archive-enhanced.csv\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07897d88",
   "metadata": {},
   "source": [
    "Import tsv file from url using format explained in this [link](https://www.adamsmith.haus/python/answers/how-to-download-a-csv-file-from-a-url-in-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9227a6",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "`r = requests.get(\"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dece0ac",
   "metadata": {},
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0536f",
   "metadata": {},
   "source": [
    "I assessed the data visually and programmatically, for quality and tidiness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141b527",
   "metadata": {},
   "source": [
    "I first viewed my tables in Microsoft Excel, where I could already see some mess in my data. Here is a sample of the original, uncleaned twitter archive enhanced.csv data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4331fd",
   "metadata": {},
   "source": [
    "![Snippet of uncleaned twitter_archive_enhaced](twitter_arc_img_snippet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfb03c",
   "metadata": {},
   "source": [
    "With this I could spot some tidiness and quality issues: __None__ in name column and the last 4 columns doing the same thing should be collapsed into one. Also, I wondered if some dogs has multiple stages, among other things. I made a note of some other questions and continued to programmatically evaluate them. Below are some of the code snippets used in assessing the different tables. I find `.info()` to be a very useful function for assessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e5890",
   "metadata": {},
   "source": [
    "![Programmatic Assessment Snippets](assessing_programmatically_snippets.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2bba28",
   "metadata": {},
   "source": [
    "##### After assessing here are some quality and tidiness issues I spotted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c1b77",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "##### `twitter_arc` table\n",
    "- Remove retweet records/rows from twitter_arc df.\n",
    "- Source columns can be easier on the eye extract the main source text eg *\"Twitter for ipone\"*\n",
    "- Wrong selected dog names like ('a','one','very'...) in `name` column.\n",
    "- Erroneous datatypes for timestamp column.\n",
    "\n",
    "##### `img_predict` table\n",
    "- Missing records in the entire table img_predict contains 2075 rows  while twitter_arc contains 2356 rows\n",
    "\n",
    "#### Tidiness\n",
    "- twitter_arc table should be joined to df_popularity table and img_predict\n",
    "- doggo, floofer, puppo and pupper should be in one column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386131cb",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c75b8",
   "metadata": {},
   "source": [
    "I took care to make duplicates of the original dataframes so that I could work on those during Cleaning rather than the original dataframes. I filled in some missing values and deleted any columns I wouldn't require for my analysis. Using the `.astype()` method, I modified the datatypes of a few columns. The image below displays how I pulled the primary source text out of the source column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d556a",
   "metadata": {},
   "source": [
    "![Extracting `source` text from source column](cleaning_source_snippet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3899e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
